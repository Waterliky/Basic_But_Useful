{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# It seems that text preprocessing is the first step for many deep learning work\n",
    "## So, I'd better wrire down those basic methods I foten used\n",
    "\n",
    "# allow foreign languages in the text\n",
    "import sys  \n",
    "stdout = sys.stdout\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "## MORE RESOURCES HERE:\n",
    "# For more, check my code files here: https://github.com/hanhanwu/Hanhan_Play_With_Social_Media\n",
    "# Sapcy modules for download: https://spacy.io/usage/models\n",
    "# Spacy tutorial: https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/\n",
    "# Textacy for preproecssing, feature extraction, text summarization with LDA, NMF, LSA:\n",
    "## https://github.com/hanhanwu/Hanhan_NLP/blob/master/textacy_explore/go_through_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"\"\"My life is so busy when it's close to the end of the year. \n",
    "        Most of the time I have to work day and night just in order to have my own time on weekends.\n",
    "        Then during the weekends, I need to sleep more, and do more house work, go shopping, \n",
    "        sometimes hag out with girlfirends. Sometimes just want to read a book, and I need to do my side projects\n",
    "        all the time. Sometimes, even during the weekend, had to work since I want to be outstanding.\n",
    "        I want to learn broader and deeper in cyber security, and I'm taking actions, I'm using my little time\n",
    "        to learn penetration testing, to learn from hackers in DEF CON. I want to be excellent and always have\n",
    "        my passion in data science in cyber secuirty. There are many other things to deal with, some are really annoying.\n",
    "        They keep remind you, life can never be perfect. I still love Emmanuel, miss him and sometimes wonder what's\n",
    "        happening there, but my busy life does not allow me to think too much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are many other things to deal with, some are really annoying.\n",
      "i still love emmanuel, miss him and sometimes wonder what's\n",
      "        happening there, but my busy life does not allow me to think too much.\n"
     ]
    }
   ],
   "source": [
    "# extract sentences from text, and lower all the case\n",
    "sentences = [s.lower() for s in nltk.tokenize.sent_tokenize(s1)]\n",
    "print sentences[7]\n",
    "print sentences[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my life is so busy when its close to the end of the year', 'most of the time i have to work day and night just in order to have my own time on weekends', 'then during the weekends i need to sleep more and do more house work go shopping \\n        sometimes hag out with girlfirends', 'sometimes just want to read a book and i need to do my side projects\\n        all the time', 'sometimes even during the weekend had to work since i want to be outstanding', 'i want to learn broader and deeper in cyber security and im taking actions im using my little time\\n        to learn penetration testing to learn from hackers in def con', 'i want to be excellent and always have\\n        my passion in data science in cyber secuirty', 'there are many other things to deal with some are really annoying', 'they keep remind you life can never be perfect', 'i still love emmanuel miss him and sometimes wonder whats\\n        happening there but my busy life does not allow me to think too much']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "# With this method, can't convter to cant;\n",
    "# With other nltk method, can't convert to can, t 2 tokens\n",
    "import string\n",
    "\n",
    "sentences = [s.translate(None, string.punctuation) for s in sentences]\n",
    "print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "order\n"
     ]
    }
   ],
   "source": [
    "# extract tokens (non-stopwords) from sentences\n",
    "tokens = [w for s in sentences for w in s.split() if w not in stopwords and len(w)<20]\n",
    "print tokens[7]\n",
    "print tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# calculate words similarity\n",
    "import Levenshtein\n",
    "\n",
    "print Levenshtein.distance(tokens[7], tokens[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n",
      "['still', 'love', 'emmanuel', u'miss', u'sometim', 'wonder', u'what', u'happen', u'busi', 'life', 'allow', 'think', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "print tokens9\n",
    "stemmed_words = [stemmer.stem(tokens9[i]) for i in range(len(tokens9))]\n",
    "print stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['things',\n",
       " 'hackers',\n",
       " 'emmanuel',\n",
       " 'actions',\n",
       " 'year',\n",
       " 'girlfirends',\n",
       " 'weekend',\n",
       " 'weekends',\n",
       " 'book',\n",
       " 'passion',\n",
       " 'def con',\n",
       " 'life',\n",
       " 'shopping',\n",
       " 'cyber secuirty',\n",
       " \"what's\",\n",
       " 'night',\n",
       " 'penetration',\n",
       " 'house work',\n",
       " 'end',\n",
       " 'data',\n",
       " 'day',\n",
       " 'projects',\n",
       " 'science',\n",
       " 'side',\n",
       " 'remind',\n",
       " 'time',\n",
       " 'security',\n",
       " 'order']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get entities, here it's just NN entities\n",
    "\n",
    "def get_NN_entities(query):\n",
    "    sentences = nltk.tokenize.sent_tokenize(query)\n",
    "    token_sets = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "    pos_tagged_token_sets = [nltk.pos_tag(t) for t in token_sets]\n",
    "    pos_tagged_tokens = [t for v in pos_tagged_token_sets for t in v]\n",
    "    \n",
    "    all_entities = []\n",
    "    previous_pos = None\n",
    "    current_entities = []\n",
    "    for (entity, pos) in pos_tagged_tokens:\n",
    "        if previous_pos == pos and pos.startswith('NN'):  # here you define entities you want\n",
    "            current_entities.append(entity.lower())\n",
    "        elif pos.startswith('NN'):  # here you define entities you want\n",
    "            if current_entities != []:\n",
    "                all_entities.append(' '.join(current_entities))\n",
    "            current_entities = [entity.lower()]\n",
    "        previous_pos = pos\n",
    "    return list(set([entity for entity in all_entities]))\n",
    "\n",
    "get_NN_entities(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My life nsubj is\n",
      "it nsubj 's\n",
      "the end pobj to\n",
      "the year pobj of\n",
      "the time pobj of\n",
      "I nsubj have\n",
      "order pobj in\n",
      "my own time dobj have\n",
      "weekends pobj on\n",
      "the weekends pobj during\n",
      "I nsubj need\n",
      "more house work dobj do\n",
      "shopping dobj go\n",
      "girlfirends pobj with\n",
      "a book dobj read\n",
      "I nsubj need\n",
      "my side projects dobj do\n",
      "the weekend pobj during\n",
      "I nsubj want\n",
      "I nsubj want\n",
      "cyber security pobj in\n",
      "I nsubj taking\n",
      "actions dobj taking\n",
      "I nsubj using\n",
      "my little time dobj using\n",
      "penetration testing dobj learn\n",
      "hackers pobj from\n",
      "DEF CON pobj in\n",
      "I nsubj want\n",
      "my passion dobj have\n",
      "data science pobj in\n",
      "cyber secuirty pobj in\n",
      "many other things attr are\n",
      "They nsubj keep\n",
      "you dobj remind\n",
      "life nsubj be\n",
      "I nsubj love\n",
      "Emmanuel dobj love\n",
      "him dobj miss\n",
      "what nsubj 's\n",
      "my busy life nsubj allow\n",
      "me nsubj think\n"
     ]
    }
   ],
   "source": [
    "# Get NN entities with Spacy\n",
    "doc = nlp(s1.decode('utf8'))\n",
    "for np in doc.noun_chunks:\n",
    "    print np.text, np.root.dep_, np.root.head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n",
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation - NLTK method\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "print tokens9\n",
    "lemmatised_words = [wordnet_lemmatizer.lemmatize(tokens9[i]) for i in range(len(tokens9))]\n",
    "print lemmatised_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, u'i')\n",
      "(still, u'still')\n",
      "(love, u'love')\n",
      "(emmanuel, u'emmanuel')\n",
      "(miss, u'miss')\n",
      "(him, u'-PRON-')\n",
      "(and, u'and')\n",
      "(sometimes, u'sometimes')\n",
      "(wonder, u'wonder')\n",
      "(what, u'what')\n",
      "(s, u's')\n",
      "(\n",
      "        , u'\\n        ')\n",
      "(happening, u'happen')\n",
      "(there, u'there')\n",
      "(but, u'but')\n",
      "(my, u'-PRON-')\n",
      "(busy, u'busy')\n",
      "(life, u'life')\n",
      "(does, u'do')\n",
      "(not, u'not')\n",
      "(allow, u'allow')\n",
      "(me, u'-PRON-')\n",
      "(to, u'to')\n",
      "(think, u'think')\n",
      "(too, u'too')\n",
      "(much, u'much')\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation - spacy method\n",
    "# type `python -m spacy download en` in your terminal to download spacy english\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en\")\n",
    "\n",
    "for token in nlp(sentences[9].decode('utf8')):\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word embeding - word2vector\n",
    "\n",
    "## Spacy\n",
    "# type `python -m spacy download en_core_web_lg`\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "## gensim\n",
    "#!pip install gensim\n",
    "# download Google pre-trained word vectors: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
    "print word_vectors[tokens9[7]]\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "tokens7 = [w for w in sentences[7].split() if w not in stopwords and len(w)<20]\n",
    "sentence=[tokens7, tokens9]\n",
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\n",
    "\n",
    "## Spark word2vector\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(there, u'ADV')\n",
      "(are, u'VERB')\n",
      "(many, u'ADJ')\n",
      "(other, u'ADJ')\n",
      "(things, u'NOUN')\n",
      "(to, u'PART')\n",
      "(deal, u'VERB')\n",
      "(with, u'ADP')\n",
      "(some, u'DET')\n",
      "(are, u'VERB')\n",
      "(really, u'ADV')\n",
      "(annoying, u'ADJ')\n",
      "(i, u'PRON')\n",
      "(still, u'ADV')\n",
      "(love, u'VERB')\n",
      "(emmanuel, u'NOUN')\n",
      "(miss, u'VERB')\n",
      "(him, u'PRON')\n",
      "(and, u'CCONJ')\n",
      "(sometimes, u'ADV')\n",
      "(wonder, u'VERB')\n",
      "(what, u'NOUN')\n",
      "(s, u'VERB')\n",
      "(\n",
      "        , u'SPACE')\n",
      "(happening, u'VERB')\n",
      "(there, u'ADV')\n",
      "(but, u'CCONJ')\n",
      "(my, u'ADJ')\n",
      "(busy, u'ADJ')\n",
      "(life, u'NOUN')\n",
      "(does, u'VERB')\n",
      "(not, u'ADV')\n",
      "(allow, u'VERB')\n",
      "(me, u'PRON')\n",
      "(to, u'PART')\n",
      "(think, u'VERB')\n",
      "(too, u'ADV')\n",
      "(much, u'ADV')\n"
     ]
    }
   ],
   "source": [
    "# part-of_speech tagging\n",
    "\n",
    "for token in nlp(sentences[7].decode('utf8')):\n",
    "    print(token,token.pos_)\n",
    "    \n",
    "for token in nlp(sentences[9].decode('utf8')):\n",
    "    print(token,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the end of the year DATE\n",
      "the weekend DATE\n",
      "DEF CON ORG\n",
      "Emmanuel PERSON\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Reognition\n",
    "\n",
    "doc = nlp(s1.decode('utf8'))\n",
    "for ent in doc.ents:\n",
    "    if ent.text.strip() != '' and ent.text is not None:\n",
    "        print ent.text, ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"And also lucky, that yesterday, I watched Star Wars when before it became a heavy rain.\\nthe theater just for Star Wars.\\nI'm still a big fan of Star Wars, it's just this one did not make me feel excited.\\npeople still love Star Wars.\\nMy favorite Star Wars are I, II, III, I like both the actors and the stories there.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text summarization\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "s2 = \"\"\"It's raining whole day today. Fortunately, I ordered all the furnitures yesterday when it was not raining. \n",
    "     And also lucky, that yesterday, I watched Star Wars when before it became a heavy rain. So many people went to\n",
    "     the theater just for Star Wars. They used multiple largest room for this movie and you even cannnot see any \n",
    "     empty seat there. But to my surprise, after the movie, people have left large amount of trash in the theater. \n",
    "     I'm still a big fan of Star Wars, it's just this one did not make me feel excited. Not sure why I just feel the\n",
    "     original feeling about destiny has become subtle in this movie and the way it captures emotions has also become\n",
    "     less careful. Jokes added in the movie just like many other Disney movies, but can no longer make me feel it's a\n",
    "     legend. It's barrative is also predictable and won't make me feel that I want to know what will happen next. Anyway\n",
    "     people still love Star Wars. My favorite Star Wars are I, II, III, I like both the actors and the stories there.\n",
    "     This year, near the end of the year, I allowed myself to watch some movie since there are really some great movies\n",
    "     at the time and working all the time did make me feel stressful. Anyway, guess this year my favorite movie is CoCo,\n",
    "     especially love that song, Remember Me. \n",
    "     What is Baby Emmanuel doing at this moment? I was surprised that till now, the news is still reporting the wild\n",
    "     fire in LA. It's winter here now and it rains heavily here, but in LA, it's still fighting the huge wild fire...\n",
    "     Christmas is coming, this year unfortunately, I cannot go back home, I'm feeling sad but keep positive, I will\n",
    "     have my furnitures delivered, get some really good desert and books, I can enjoy the Christmas by reading the books\n",
    "     enjoying the desert. I miss Baby Emmauel so much, would he know, I always think it's very cool that he likes playing\n",
    "     video games?\"\"\"\n",
    "\n",
    "summarize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[And also lucky, that yesterday, I watched Star Wars when before it became a heavy rain., So many people went to     the theater just for Star Wars., I'm still a big fan of Star Wars, Anyway     people still love Star Wars., My favorite Star Wars are I, II, III, I like both the actors and the stories there.     ]\n",
      "************************\n",
      "And : [lucky]\n",
      "also : []\n",
      "lucky : [also, ,]\n",
      ", : []\n",
      "that : []\n",
      "yesterday : []\n",
      ", : []\n",
      "I : []\n",
      "watched : [And, that, yesterday, ,, I, Wars, became, .]\n",
      "Star : []\n",
      "Wars : [Star]\n",
      "when : []\n",
      "before : []\n",
      "it : []\n",
      "became : [when, before, it, rain]\n",
      "a : []\n",
      "heavy : []\n",
      "rain : [a, heavy]\n",
      ". : []\n",
      "************************\n",
      "So : []\n",
      "many : [So]\n",
      "people : [many]\n",
      "went : [people, to, theater, .]\n",
      "to : [    ]\n",
      "     : []\n",
      "the : []\n",
      "theater : [the, for]\n",
      "just : []\n",
      "for : [just, Wars]\n",
      "Star : []\n",
      "Wars : [Star]\n",
      ". : []\n",
      "************************\n",
      "I : []\n",
      "'m : [I, still, fan]\n",
      "still : []\n",
      "a : []\n",
      "big : []\n",
      "fan : [a, big, of]\n",
      "of : [Wars]\n",
      "Star : []\n",
      "Wars : [Star]\n",
      "************************\n",
      "Anyway : [    ]\n",
      "     : []\n",
      "people : []\n",
      "still : []\n",
      "love : [Anyway, people, still, Wars, .]\n",
      "Star : []\n",
      "Wars : [Star]\n",
      ". : []\n",
      "************************\n",
      "My : []\n",
      "favorite : []\n",
      "Star : []\n",
      "Wars : [My, favorite, Star]\n",
      "are : [Wars, I, like]\n",
      "I : [II]\n",
      ", : []\n",
      "II : [,, ,, III]\n",
      ", : []\n",
      "III : []\n",
      ", : []\n",
      "I : []\n",
      "like : [,, I, actors, .]\n",
      "both : []\n",
      "the : []\n",
      "actors : [both, the, and, stories]\n",
      "and : []\n",
      "the : []\n",
      "stories : [the, there]\n",
      "there : []\n",
      ". : [    ]\n",
      "     : []\n"
     ]
    }
   ],
   "source": [
    "# Dependency Parsing\n",
    "doc = nlp(''.join(s2.split('\\n')).decode('utf8'))\n",
    "star_wars_sentences = [sent for sent in doc.sents if 'star wars' in sent.string.lower()]\n",
    "print star_wars_sentences\n",
    "\n",
    "for sent in star_wars_sentences:\n",
    "    print \"************************\"\n",
    "    for wd in sent:\n",
    "        print wd, ':', str(list(wd.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u\"Emmanuel is very adorable!--------------------------------------- {'neg': 0.0, 'neu': 0.442, 'pos': 0.558, 'compound': 0.5838}\", u\"Hanhan's label:pos\")\n",
      "(u\"Baby Emmanuel also likes Star Wars and Totoro.------------------- {'neg': 0.29, 'neu': 0.484, 'pos': 0.226, 'compound': -0.2023}\", u\"Hanhan's label:pos\")\n",
      "(u\"I love ice-cream------------------------------------------------- {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\", u\"Hanhan's label:pos\")\n",
      "(u\"Christmas is coming!--------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:pos\")\n",
      "(u\"Baby Emmanuel will return back to my life.----------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:pos\")\n",
      "(u\"Baby Emmauel often hurt himself for no reason.------------------- {'neg': 0.483, 'neu': 0.517, 'pos': 0.0, 'compound': -0.6808}\", u\"Hanhan's label:neg\")\n",
      "(u\"Emmanuel does not want to see me again--------------------------- {'neg': 0.149, 'neu': 0.851, 'pos': 0.0, 'compound': -0.0572}\", u\"Hanhan's label:neg\")\n",
      "(u\"I can't go home this year---------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:neg\")\n",
      "(u\"Baby Emmanuel is gaining more fat-------------------------------- {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\", u\"Hanhan's label:neg\")\n",
      "(u\"Baby Emmanuel forgot me------------------------------------------ {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:neg\")\n",
      "(u\"Totoro likes baby Emmanuel--------------------------------------- {'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}\", u\"Hanhan's label:pos\")\n",
      "(u\"Christmas is coming, but I cannot go back home.------------------ {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:neg\")\n",
      "(u\"Tomorrow is Monday again.---------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\", u\"Hanhan's label:neg\")\n",
      "(u\"Holiday is coming!----------------------------------------------- {'neg': 0.0, 'neu': 0.401, 'pos': 0.599, 'compound': 0.4574}\", u\"Hanhan's label:pos\")\n",
      "(u\"Baby Emmanuel is happy.------------------------------------------ {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.5719}\", u\"Hanhan's label:pos\")\n",
      "(u\"LA has wild fire.------------------------------------------------ {'neg': 0.444, 'neu': 0.556, 'pos': 0.0, 'compound': -0.34}\", u\"Hanhan's label:neg\")\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentences = [('Emmanuel is very adorable!', 'pos'),          \n",
    "         ('Baby Emmanuel also likes Star Wars and Totoro.', 'pos'),\n",
    "         ('I love ice-cream', 'pos'),\n",
    "         ('Christmas is coming!', 'pos'),\n",
    "         (\"Baby Emmanuel will return back to my life.\", 'pos'),\n",
    "         ('Baby Emmauel often hurt himself for no reason.', 'neg'),\n",
    "         ('Emmanuel does not want to see me again', 'neg'),\n",
    "         (\"I can't go home this year\", 'neg'),\n",
    "         ('Baby Emmanuel is gaining more fat', 'neg'),          \n",
    "         ('Baby Emmanuel forgot me', 'neg'),\n",
    "         ('Totoro likes baby Emmanuel', 'pos'),     \n",
    "         ('Christmas is coming, but I cannot go back home.', 'neg'),\n",
    "         (\"Tomorrow is Monday again.\", 'neg'),\n",
    "         (\"Holiday is coming!\", 'pos'),\n",
    "         ('Baby Emmanuel is happy.', 'pos'),\n",
    "         (\"LA has wild fire.\", 'neg')]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence[0])\n",
    "    print(\"{:-<65} {}\".format(sentence[0], str(vs)), \"Hanhan's label:\"  + sentence[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
