{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# It seems that text preprocessing is the first step for many deep learning work\n",
    "## So, I'd better wrire down those basic methods I foten used\n",
    "\n",
    "# allow foreign languages in the text\n",
    "import sys  \n",
    "stdout = sys.stdout\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For more, check my code files here: https://github.com/hanhanwu/Hanhan_Play_With_Social_Media\n",
    "# Sapcy modules for download: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"\"\"My life is so busy when it's close to the end of the year. \n",
    "        Most of the time I have to work day and night just in order to have my own time on weekends.\n",
    "        Then during the weekends, I need to sleep more, and do more house work, go shopping, \n",
    "        sometimes hag out with girlfirends. Sometimes just want to read a book, and I need to do my side projects\n",
    "        all the time. Sometimes, even during the weekend, had to work since I want to be outstanding.\n",
    "        I want to learn broader and deeper in cyber security, and I'm taking actions, I'm using my little time\n",
    "        to learn penetration testing, to learn from hackers in DEF CON. I want to be excellent and always have\n",
    "        my passion in data science in cyber secuirty. There are many other things to deal with, some are really annoying.\n",
    "        They keep remind you, life can never be perfect. I still love Emmanuel, miss him and sometimes wonder what's\n",
    "        happening there, but my busy life does not allow me to think too much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are many other things to deal with, some are really annoying.\n",
      "i still love emmanuel, miss him and sometimes wonder what's\n",
      "        happening there, but my busy life does not allow me to think too much.\n"
     ]
    }
   ],
   "source": [
    "# extract sentences from text, and lower all the case\n",
    "sentences = [s.lower() for s in nltk.tokenize.sent_tokenize(s1)]\n",
    "print sentences[7]\n",
    "print sentences[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my life is so busy when its close to the end of the year', 'most of the time i have to work day and night just in order to have my own time on weekends', 'then during the weekends i need to sleep more and do more house work go shopping \\n        sometimes hag out with girlfirends', 'sometimes just want to read a book and i need to do my side projects\\n        all the time', 'sometimes even during the weekend had to work since i want to be outstanding', 'i want to learn broader and deeper in cyber security and im taking actions im using my little time\\n        to learn penetration testing to learn from hackers in def con', 'i want to be excellent and always have\\n        my passion in data science in cyber secuirty', 'there are many other things to deal with some are really annoying', 'they keep remind you life can never be perfect', 'i still love emmanuel miss him and sometimes wonder whats\\n        happening there but my busy life does not allow me to think too much']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "# With this method, can't convter to cant;\n",
    "# With other nltk method, can't convert to can, t 2 tokens\n",
    "import string\n",
    "\n",
    "sentences = [s.translate(None, string.punctuation) for s in sentences]\n",
    "print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "order\n"
     ]
    }
   ],
   "source": [
    "# extract tokens (non-stopwords) from sentences\n",
    "tokens = [w for s in sentences for w in s.split() if w not in stopwords and len(w)<20]\n",
    "print tokens[7]\n",
    "print tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# calculate words similarity\n",
    "import Levenshtein\n",
    "\n",
    "print Levenshtein.distance(tokens[7], tokens[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n",
      "['still', 'love', 'emmanuel', u'miss', u'sometim', 'wonder', u'what', u'happen', u'busi', 'life', 'allow', 'think', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "print tokens9\n",
    "stemmed_words = [stemmer.stem(tokens9[i]) for i in range(len(tokens9))]\n",
    "print stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['things',\n",
       " 'hackers',\n",
       " 'emmanuel',\n",
       " 'actions',\n",
       " 'year',\n",
       " 'girlfirends',\n",
       " 'weekend',\n",
       " 'weekends',\n",
       " 'book',\n",
       " 'passion',\n",
       " 'def con',\n",
       " 'life',\n",
       " 'shopping',\n",
       " 'cyber secuirty',\n",
       " \"what's\",\n",
       " 'night',\n",
       " 'penetration',\n",
       " 'house work',\n",
       " 'end',\n",
       " 'data',\n",
       " 'day',\n",
       " 'projects',\n",
       " 'science',\n",
       " 'side',\n",
       " 'remind',\n",
       " 'time',\n",
       " 'security',\n",
       " 'order']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get entities, here it's just NN entities\n",
    "\n",
    "def get_NN_entities(query):\n",
    "    sentences = nltk.tokenize.sent_tokenize(query)\n",
    "    token_sets = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "    pos_tagged_token_sets = [nltk.pos_tag(t) for t in token_sets]\n",
    "    pos_tagged_tokens = [t for v in pos_tagged_token_sets for t in v]\n",
    "    \n",
    "    all_entities = []\n",
    "    previous_pos = None\n",
    "    current_entities = []\n",
    "    for (entity, pos) in pos_tagged_tokens:\n",
    "        if previous_pos == pos and pos.startswith('NN'):  # here you define entities you want\n",
    "            current_entities.append(entity.lower())\n",
    "        elif pos.startswith('NN'):  # here you define entities you want\n",
    "            if current_entities != []:\n",
    "                all_entities.append(' '.join(current_entities))\n",
    "            current_entities = [entity.lower()]\n",
    "        previous_pos = pos\n",
    "    return list(set([entity for entity in all_entities]))\n",
    "\n",
    "get_NN_entities(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n",
      "['still', 'love', 'emmanuel', 'miss', 'sometimes', 'wonder', 'whats', 'happening', 'busy', 'life', 'allow', 'think', 'much']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation - NLTK method\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "print tokens9\n",
    "lemmatised_words = [wordnet_lemmatizer.lemmatize(tokens9[i]) for i in range(len(tokens9))]\n",
    "print lemmatised_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, u'i')\n",
      "(still, u'still')\n",
      "(love, u'love')\n",
      "(emmanuel, u'emmanuel')\n",
      "(miss, u'miss')\n",
      "(him, u'-PRON-')\n",
      "(and, u'and')\n",
      "(sometimes, u'sometimes')\n",
      "(wonder, u'wonder')\n",
      "(what, u'what')\n",
      "(s, u's')\n",
      "(\n",
      "        , u'\\n        ')\n",
      "(happening, u'happen')\n",
      "(there, u'there')\n",
      "(but, u'but')\n",
      "(my, u'-PRON-')\n",
      "(busy, u'busy')\n",
      "(life, u'life')\n",
      "(does, u'do')\n",
      "(not, u'not')\n",
      "(allow, u'allow')\n",
      "(me, u'-PRON-')\n",
      "(to, u'to')\n",
      "(think, u'think')\n",
      "(too, u'too')\n",
      "(much, u'much')\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation - spacy method\n",
    "# type `python -m spacy download en` in your terminal to download spacy english\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en\")\n",
    "\n",
    "for token in nlp(sentences[9].decode('utf8')):\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word embeding - word2vector\n",
    "\n",
    "## Spacy\n",
    "# type `python -m spacy download en_core_web_lg`\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "\n",
    "## gensim\n",
    "#!pip install gensim\n",
    "# download Google pre-trained word vectors: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\n",
    "print word_vectors[tokens9[7]]\n",
    "tokens9 = [w for w in sentences[9].split() if w not in stopwords and len(w)<20]\n",
    "tokens7 = [w for w in sentences[7].split() if w not in stopwords and len(w)<20]\n",
    "sentence=[tokens7, tokens9]\n",
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\n",
    "\n",
    "## Spark word2vector\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
